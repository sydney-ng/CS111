NAME: Sydney Ng
EMAIL: sydney.ng@yahoo.com
ID: 804794021
SLIPDAYS: 4

Project Description: 

	This is a project that uses different methods of synchronizations such as locks to examine how they affect the accuracy and scalability of a program. For this example, we examine mutexes, spin locks, and compare and swap locks and how it is used to preserve the integrity of a linked list when we have many threads or iterations. We also look at the affect that yielding has on the outcome of the processes and how that affects the distribution and timing of resources.  

QUESTION 2.3.1 - CPU time in the basic list implementation:
	There are 8 different cases to consider for this question. If there is 1 thread and small number of iterations or large number of iterations for spinlock, majority of the time will be spent in the list operation as there is only 1 thread to do the computation and there is no wait time. The one thread will always be able to obtain the resource. For 2 threads and small number of iterations or large number of iterations for spinlock, half the time will be spent in spinning and the other half will be in the liste operations. This is because whenever one thread is doing the computation, the other thread has to spin. The number of iterations doesn't matter becuase the graph will just look the same, just longer because one thread will always be waiting. If there is 1 thread or 2 threads and a small number of iterations for mutex, we there is no way of knowing exactly where the time is spent. We don't have a way to record how fast the mutex is obtained and released & even if we did, this number could be not accurate. If there is 1 thread or 2 threads and a large number of iterations for mutex, majority of the time will be spent in completing the list operation because there are many nodes that we need to traverse and check through. 
QUESTION 2.3.2 - Execution Profiling:
	My set_spinlock_lock function took the most amount of time when the spin-lock version of the list exerciser is run with a large number of threads. This is because this parent function is the one that will call __sync_lock_test_and_set_ and will also eventually call __sync_lock_release, so many locks spend a lot of time in this function when they are between stages of doing the computation. This happens before the insert, length, and lookup/delete operations, which is why this time is multiplied by three. 
QUESTION 2.3.3 - Mutex Wait Time:
	The average lock-wait time rises dramatically with the number of contending threads because you have more threads competeing for the same resources. Because one thread has the lock, the rest of the threads must wait, so if you have more threads, that's more threads waiting. The completion time per operation also rises with the number of contending threads because the wall time will add up when you have 8 or 16 threads, giving you a higher completion time. However, because completion time doesn't include wait time, it is possible for you to have the wait time per operation to go up faster (or higher) than the completion time per operation. 
QUESTION 2.3.4 - Performance of Partitioned Lists: 
	In terms of performance, you're splitting up the lists into smaller lists so that you can use a simple hashing function to find out what smaller list it is in. This decreases the lookup, insertion, and length calculation time. The throughput would continue increasing as the number of lists is further increased because you would be doing what was said above. However, this eventually would level off because you'll just have one element in every sub-list. An N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads becuase the number of sublists is directly correlated with the number of threads that have been created in a specific process. However, this is not reflected in the curves.