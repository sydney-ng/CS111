NAME: Sydney Ng
EMAIL: sydney.ng@yahoo.com
ID: 804794021
SLIPDAYS: 2

Project Description: 

	This is a project that uses different methods of synchronizations such as locks to examine how they affect the accuracy and scalability of a program. For this example, we examine mutexes, spin locks, and compare and swap locks and how it is used to preserve the integrity of the data when we have many threads or iterations. We also look at the affect that yielding has on the outcome of the processes and how that affects the distribution and timing of resources.  

QUESTION 2.1.1 - causing conflicts:
	It takes about 10000 iterations and you will begin to see errors. You’ll see less errors on smaller iterations because there is less competition for resources. Just by the principles of probability, if you exponentially increase the amount of error that could happen when 2 (or in this case 10000 computations are trying to happen as synchronously as possible.
QUESTION 2.1.2 - cost of yielding:
	Yield is significantly slower because according to the man page for sched_yield(), this function “causes the calling thread to relinquish the CPU”. There is a context switch forcing the processor to complete the tasks for another thread. For smaller programs, this doesn’t have as large of an affect because the overhead is proportionally smaller. When you have many iterations, however, this increase in the number of context switching exponentially affects the runtimes.The man pages specify that the current thread is put at the “end of the queue for its static priority and a new thread gets to run”. This means that there is a context switch. During this context switch, it takes time to process an interrupt, save all the values from the context of that process (ex. caches, stack, registers). You also are switching from user to kernel mode and asking the scheduler to rearrange the order in which the threads will be processes. All of these steps require time, and when you multiply this by N number of threads, the additional time will increase exponentially. No, it is not possible to get valid per-operation timings. When you call yield, you are asking the scheduler to switch from your current process to another. However, when you do this, you are adding time for the interrupts, switching form user to kernel mode, and all of the steps that were mentioned in 2.1.2.2. This is taken account into the total runtime that we use to get the per-operation timings. Also, when you are doing this type of operations, the processor may not be able to use all of the optimizations it otherwise would if it had more resources for a longer period of time (therefore also increasing the time that it takes for that thread job to complete). 
QUESTION 2.1.3 - measurement errors:
	The average cost per operation will drop with increasing iterations because there is a large overhead that is associated with created a thread. Each time you create a thread, you need to create and save variables that correspond with the program resources and execution state. In addition, each thread has its own stack pointer, registers, scheduling priorities, and signal statuses. Threads that are in the same process can share resources, but they still operate independently so it is still costly. Therefore, if you can maximize the number of iterations that you are doing, you can help balance out these costs. Eventually, the graph levels out after ___number of iterations. You can then use simple math to do (upper bound number of iterations - lower bound number of iterations)/(time 2 - time 1). This would give you a close number . 
QUESTION 2.1.4 - costs of serialization:
	When you have low number of threads, there is less competition to get the lock. Therefore, this increases the probability that you will get the lock from the program. However, when you increase the number of threads, there are more race conditions. This is more opportunity for calculations to  happen not synchronously. The protected operations slow down as the number of threads rises because you have lots of threads that need to use the lock. Once a thread has hold of a lock, it will not relinquish it until the operation that needs to be performed has been completed. Therefore, if you have lots of threads that are holding the lock, you will see the efficiency and speed of the program dramatically decrease. 
QUESTION 2.2.1 - scalability of Mutex:
	Time per mutex-protected operation seems to be a lot slower in Part 1. This is because in the second part, we are multiplying the number of threads x iterations by 3 since we have to lookup,  delete, and insert. This increases the number of operations that occur (while the first part only had threads x iterations). By adding more steps, this will make the mutex-protected operations a lot faster. 
	QUESTION 2.2.2 - scalability of spin locks: 
	In the beginning the shapes of the curves look very similar for mutex and spinlocks. This is because there are not many race conditions as there aren’t that many threads that need to acquire the lock. However, when we increase the number of threads, this is when we can see the difference. While this wouldn’t matter much in the beginning when there are not many threads, spinlocks repeatedly check to see if it can grab the lock and this content checking multiplied by a large number of threads will make it take a long time. Mutexes work more straightforwardly with just a simple lock and unlock. In the beginning, again this may be more costly (and therefor similar to using a spinlock). However, if you have a lot of threads, it is useful to just have this straightforward method instead of having to check each time. We can see this reflected in the curve of the graph with Mutex completing faster. 



